# -*- coding: utf-8 -*-
"""

@author: japeach

Conversion of TELMOS2_v2.2 vb scripts
"""

import os
from itertools import product
from typing import Dict
from typing import Union
from collections import defaultdict

import numpy as np
import pandas as pd

from scripts.extract_trip_rates import convert_rates_format

MALE_STUDENT_FACTOR = 0.2794
FEMALE_STUDENT_FACTOR = 0.2453

# Define the values used when reading in trip rates
TR_PURPOSES = ["HBW", "HBO", "HBE", "HBS"]
TR_MODES = ["Car", "PT"]
TR_PERIODS = ["AM", "IP", "PM"]
TR_AREA_TYPES = list(range(3, 9))
TR_WORK_TYPES = ["WAH", "WBC"]


def read_trip_rates(factors_dir, just_pivots):
    '''
    Loads the production trip rate files into a numpy array
    '''
    
    periods = TR_PERIODS
    
    if just_pivots is True:
        periods.append("OP")
    
    sr_array = []
    for area_type in TR_AREA_TYPES:
        data = []
        segmentations = product(periods, TR_PURPOSES, TR_MODES)
        for period, purpose, mode in segmentations:
            file_name = "%s_%s_%s_%s.txt" % (purpose, mode, period, area_type)
            factor_file = os.path.join(factors_dir, file_name)
            data.append(np.loadtxt(factor_file))
        sr_array.append(data)
    return np.asarray(sr_array)


def read_trip_rates_home_working(factors_dir: str,
                                 just_pivots: bool,
                                 wah_tag: str = "WAH"
                                 ) -> np.array:

    # Add off-peak to the period list if required
    periods = TR_PERIODS
    if just_pivots is True:
        periods.append("OP")

    file_base = "{purp}_{mode}_{period}_{area}_{wah_tag}.txt"

    # Setup an empty list to store the trip rates
    trip_rates = []

    # Loop through all available segmentation for the trip rates
    for area_type in TR_AREA_TYPES:

        data = []
        segmentations = product(periods, TR_PURPOSES, TR_MODES)

        for period, purpose, mode in segmentations:

            file_name = file_base.format(
                purp=purpose,
                mode=mode,
                period=period,
                area=area_type,
                wah_tag=wah_tag
            )
            file_path = os.path.join(factors_dir, file_name)

            data.append(np.loadtxt(file_path))

        trip_rates.append(data)

    return np.asarray(trip_rates)


def read_long_trip_rates(trip_rate_path: str,
                         work_type_split: bool = False,
                         just_pivots: bool = False
                         ) -> Union[np.array, Dict[str, np.array]]:
    """Alternative function to the original read_trip_rates and
    read_trip_rates_home_working functions. Reads in a single combined as
    generated by extract_trip_rates.py for easier input file management.

    Args:
        trip_rate_path (str): Path to the trip rate combined file.
        work_type_split (bool, optional): Flag if rates are split between
        Working at Home and Working by Commute. Defaults to False.
        just_pivots (bool, optional): Flag if only pivots are being created
        and all periods shoule be used. Defaults to False.

    Returns:
        Union[np.array, Dict[str, np.array]]: Either a np.array object if
        work_type_split is False, or a dictionary with keys ["WAH", "WBC"]
        if True.
    """

    # Define the segmentation required from the trip rate file - could move
    # this to common constants
    trip_rate_seg = ["purpose", "mode", "period", "area", "traveller_type"]
    if work_type_split:
        trip_rate_seg.append("work_type")
    trip_rate_val = ["trip_rate"]

    # Read in all trip rates as a dataframe
    tr_df = pd.read_csv(trip_rate_path)[trip_rate_seg + trip_rate_val]

    # Add off-peak to the period list if required
    periods = TR_PERIODS
    if just_pivots is True:
        periods.append("OP")

    # Setup an empty dictionary to store the rates [work_type]
    trip_rates = defaultdict(lambda: defaultdict(list))

    # Loop through all available segmentation for the trip rates
    segmentations = product(TR_AREA_TYPES, periods, TR_PURPOSES, TR_MODES)
    for area_type, period, purpose, mode in segmentations:
        # Extract the segmentation from the dataframe
        df = tr_df.loc[
            (tr_df["period"] == period)
            & (tr_df["mode"] == mode)
            & (tr_df["purpose"] == purpose)
            & (tr_df["area"] == area_type)
        ]
        # Extract just the traveller type, work type (if available) and
        # trip rate
        df = df[trip_rate_seg[4:] + trip_rate_val]

        if work_type_split:
            for work_type in TR_WORK_TYPES:
                filtered_df = df.loc[df["work_type"] == work_type]
                arr = filtered_df.sort_values("traveller_type")[trip_rate_val]
                arr = convert_rates_format(
                    arr.values,
                    direction="to_wide"
                )
                trip_rates[work_type][area_type].append(arr)
        else:
            arr = df.sort_values("traveller_type")[trip_rate_val]
            arr = convert_rates_format(
                arr.values,
                direction="to_wide"
            )
            trip_rates["ALL"][area_type].append(arr)

    # Combine into a single numpy array
    for work_type in trip_rates:
        tr_array = list(trip_rates[work_type].values())
        trip_rates[work_type] = np.asarray(tr_array)

    if work_type_split:
        return trip_rates
    else:
        return trip_rates["ALL"]


def load_cte_tod_files(tod_files, cte_files, file_base):
    tod_data = []
    cte_data = []
    for t_file, c_file in zip(tod_files, cte_files):
        # TMfS14 had a long-distance module that required some CTE/TOD files to have
        # _All appended to the end. This can now be removed if needed.
        t_file_name = (
            t_file if os.path.isfile(os.path.join(file_base, t_file))
            else t_file.upper().replace("_ALL", "")
        )
        c_file_name = (
            c_file if os.path.isfile(os.path.join(file_base, c_file))
            else c_file.upper().replace("_ALL", "")
        )
        tod_data.append(np.loadtxt(os.path.join(file_base, t_file_name),
                                   delimiter=","))
        cte_data.append(np.loadtxt(os.path.join(file_base, c_file_name),
                                   delimiter=","))
    return (np.asarray(tod_data), np.asarray(cte_data))


def student_factor_adjustment(population_data: np.array) -> np.array:
    """Split columns in the tmfs population data using student factors.
    Also removes unnecessary columns.

    Args:
        population_data (np.array): Numpy array of population data, extracted 
        from the DELTA directory.

    Returns:
        np.array: The adjusted array containing non-working split by student/
        non-students
    """
    adjusted_arr = np.copy(population_data)

    adjusted_arr[:, :3] = adjusted_arr[:, 2:5]
    adjusted_arr[:, 3] = adjusted_arr[:, 7] * MALE_STUDENT_FACTOR
    adjusted_arr[:, 4] = adjusted_arr[:, 8] * FEMALE_STUDENT_FACTOR
    adjusted_arr[:, 7] = adjusted_arr[:, 7] * (1 - MALE_STUDENT_FACTOR)
    adjusted_arr[:, 8] = adjusted_arr[:, 8] * (1 - FEMALE_STUDENT_FACTOR)

    return adjusted_arr


def create_attraction_pivot(planning_data, attraction_trip_rates):
    ''' 
    Multiplies the planning data and the attraction trip rates
    '''
    # Columns are: Work, Employment, Other, Education
    attr_factors_array = np.ones((planning_data.shape[0], 4),dtype="float32")
    attr_factors_array[:,0] = planning_data[:,2] * attraction_trip_rates[0, 0]
    attr_factors_array[:,1] = planning_data[:,[1,3,4,5,6,7,8]].sum(axis=1) * attraction_trip_rates[2,1]
    attr_factors_array[:,2] = (planning_data[:,[4,8,1,3]] * 
                      attraction_trip_rates[[6,7,1,12],[3,4,8,9]]).sum(axis=1)
    attr_factors_array[:,3] = planning_data[:,7] * attraction_trip_rates[2,2]
    return attr_factors_array

def create_production_pivot(planning_data, production_trip_rates, area_correspondence,
                            check_file, count_tav, output_shape, just_pivots):
    ''' 
    Multiplies the planning data and the production trip rates
    
    planning_data : adjusted planning data (tmfsxxxx.csv)
    production_trip_rates : array of all production trip rates (p_trip_rate_array)
    area_correspondence : array containing urban/rural classification of zones 
    check_file : output path for check2.csv file
    count_tav : number of internal zones
    output_shape : shape of pivot file (same as base pivot file)
    just_pivots : bool : if just the pivots should be output (false)
    '''
    sr_prod_array = np.zeros((24, planning_data.shape[0], 11))
    if just_pivots is True:
        sr_prod_array = np.zeros((32, planning_data.shape[0], 11))
        
    with open(check_file, "w", newline="") as check:
        trr = 0
        for k in range(sr_prod_array.shape[0]):
            for j in range(sr_prod_array.shape[2]):
                for i in range(sr_prod_array.shape[1]-1):
                    sr_prod_array[k, i, j] = (planning_data[i, j] * 
                                 production_trip_rates[area_correspondence[i]-3, k, trr, j])
                    trr += 1
                    if trr == 8:
                        trr = 0
                    check.write(str(sr_prod_array[k,i,j]))
                    check.write("\n")
                    
    prod_factor_array = np.ones(output_shape)
    
    # Just Pivots is a debug option to output an extended version of the pivoting files
    # Originally outputs just 64 columns - 2 periods * 4 Purposes * 2 Modes * 4 Household types
    column_width = 2 * 4 * 2 * 4
    if just_pivots is True:
        # If just calculating the pivoting tables output all the possible time periods
        # - 4 Periods * 4 Purposes * 2 Modes * 4 Household types
        column_width = 4 * 4 * 2 * 4
        prod_factor_array = np.zeros(
                (output_shape[0], output_shape[1] + 
                 int(column_width / 2)))
        
    # aggregates the household types into C0, C11, C12, C2
    for i in range(count_tav):
        ca = 0
        cb = 1
        cc = 2
        cd = 3
        for j in range(column_width):
            v = 0
            for b in range(11):
                if j == ca:
                    v += (sr_prod_array[int((ca+4)/4)-1,0+(i*8),b] + 
                                        sr_prod_array[int((ca+4)/4)-1,2+(i*8),b] + 
                                        sr_prod_array[int((ca+4)/4)-1,5+(i*8),b])
                if j == cb:
                    v += sr_prod_array[int((cb+3)/4)-1,1+(i*8),b]
                if j == cc:
                    v += (sr_prod_array[int((cc+2)/4)-1,3+(i*8),b] + 
                                        sr_prod_array[int((cc+2)/4)-1,6+(i*8),b])
                if j == cd:
                    v += (sr_prod_array[int((cd+1)/4)-1,4+(i*8),b] + 
                                        sr_prod_array[int((cd+1)/4)-1,7+(i*8),b])

            if j == ca:
                ca += 4
            if j == cb:
                cb += 4
            if j == cc:
                cc += 4
            if j == cd:
                cd += 4

            prod_factor_array[i, j] = v
    
    return prod_factor_array

def calculate_growth(base, forecast):
    growth = forecast / base
    growth = np.select(
        [np.isinf(growth), np.isnan(growth)],
        [forecast, 1], default=growth
    )
    return growth

def apply_pivot_files(tod_data, cte_data, 
                      production_growth, attraction_growth,
                      airport_growth):
    '''
    Applies growth to the base cte and tod files
    
    '''
    sw_array = np.zeros_like(tod_data, dtype="float")
    
    tod_attr_growth_idxs = [0,2,1,3,0,2,1,3,3]
    for j in range(sw_array.shape[0] - 1):
        sw_array[j,:,1:4] = ((cte_data[j,:,1:4] * production_growth[:,(1+8*j):(4+8*j)]) +
                            (cte_data[j,:,4:7] * production_growth[:,(5+8*j):(8+8*j)])) * airport_growth[:,None]
        sw_array[j,:,4] = tod_data[j,:,4] * production_growth[:,(4+8*j)] * airport_growth
        sw_array[j,:,5] = tod_data[j,:,5] * attraction_growth[:,tod_attr_growth_idxs[j]] * airport_growth

    sw_array[8,:,1:4] = ((cte_data[8,:,1:4] * production_growth[:,(1+8*7):(4+8*7)]) +
                            (cte_data[8,:,4:7] * production_growth[:,(5+8*7):(8+8*7)])) * airport_growth[:,None]
    sw_array[8,:,4] = tod_data[8,:,4] * production_growth[:,(4+8*7)] * airport_growth
    sw_array[8,:,5] = tod_data[8,:,5] * attraction_growth[:,tod_attr_growth_idxs[7]] * airport_growth

    sw_prod = {}
    sw_attr = {}
    sw_prod["1"] = sw_array[0,:,1:5].sum()
    sw_prod["5"] = sw_array[4,:,1:5].sum()
    sw_prod["4"] = sw_array[3,:,1:5].sum()
    sw_prod["14"] = sw_array[7,:,1:5].sum()
    sw_prod["17"] = sw_array[8,:,1:5].sum()

    sw_attr["1"] = sw_array[0,:,5].sum()
    sw_attr["5"] = sw_array[4,:,5].sum()
    sw_attr["4"] = sw_array[3,:,5].sum()
    sw_attr["14"] = sw_array[7,:,5].sum()
    sw_attr["17"] = sw_array[8,:,5].sum()

    sw_array[0,:,5] *= (sw_prod["1"] / sw_attr["1"])
    sw_array[4,:,5] *= (sw_prod["5"] / sw_attr["5"])
    sw_array[3,:,5] *= (sw_prod["4"] / sw_attr["4"])
    sw_array[7,:,5] *= (sw_prod["14"] / sw_attr["14"])
    sw_array[8,:,5] *= (sw_prod["17"] / sw_attr["17"])
    
    sw_cte_array = np.zeros_like(cte_data, dtype="float")
    prod_col_idxs = np.array([1,2,3,5,6,7,4])
    cte_attr_growth_idxs = [None, 2, 1, None, None, 2, 1, None, None]
    for j in range(sw_cte_array.shape[0]):
        if j < 8:
            sw_cte_array[j,:,1:8] = (cte_data[j,:,1:8] * 
                        production_growth[:,prod_col_idxs+(j*8)] *
                        airport_growth[:,None])
        else:
            sw_cte_array[j,:,1:8] = (cte_data[j,:,1:8] * 
                        production_growth[:,prod_col_idxs+((j-1)*8)] *
                        airport_growth[:,None])
        # These columns do the following
        if j in [0,3,4,7,8]:
            sw_cte_array[j,:,8] = (cte_data[j,:,8] * 
                        attraction_growth[:,tod_attr_growth_idxs[j]] * 
                        airport_growth)
        # Otherwise...
        else:
            sw_cte_array[j,:,8] = (cte_data[j,:,8] * 
                        attraction_growth[:,cte_attr_growth_idxs[j]] * 
                        airport_growth)
    
    return (sw_array, sw_cte_array)

def save_trip_end_files(file_names, trip_ends, base_path, precision):
    for i, t_file in zip(range(trip_ends.shape[0]), file_names):
        path = os.path.join(base_path, t_file.replace("_ALL", ""))
        np.savetxt(path, np.concatenate(
                (np.arange(trip_ends[i].shape[0])[:,None]+1, trip_ends[i][:,1:]),axis=1),
                    delimiter=", ", 
                    fmt=["%d"]+["%." + str(precision) + "f" for x in range(trip_ends[i].shape[1]-1)])
                

def telmos_main(delta_root, tmfs_root, tel_year, tel_id, tel_scenario,
                base_year, base_id, base_scenario, is_rebasing_run=True,
                log_func=print, just_pivots=False, airport_growth_file="",
                integrate_home_working=False, legacy_trip_rates=False):
    '''
    Applies growth to base year trip end files for input into the second stage 
    of the TMfS18 trip end model
    '''
    
    
    # # Read in the trip rate matrices into multi-dim array
    factors_base = os.path.join(tmfs_root, "Factors")
    if legacy_trip_rates:
        tr_message = "Loaded {} Trip Rate Factors with shape: {}"
        if integrate_home_working:
            log_func("Integrating Home Working Splits")
            p_trip_rate_array = {"WAH": None, "WBC": None}
            for work_type in p_trip_rate_array:
                p_trip_rate_array[work_type] = read_trip_rates_home_working(
                    factors_base,
                    just_pivots=just_pivots,
                    wah_tag=work_type
                )
                log_func(
                    tr_message.format(work_type, 
                                      p_trip_rate_array[work_type].shape)
                )
        else:
            p_trip_rate_array = read_trip_rates(factors_base, just_pivots)
            log_func(tr_message.format("all", p_trip_rate_array.shape))

    # Read in the combined version of the trip rate files
    else:
        tr_name = "TripRates.csv"
        if integrate_home_working:
            log_func("Integrating Home Working Splits")
            tr_name = tr_name.replace("Rates", "RatesSplit")
        tr_path = os.path.join(factors_base, tr_name)
        p_trip_rate_array = read_long_trip_rates(
            tr_path,
            work_type_split=integrate_home_working
        )
        log_func(f"Loaded Trip Rate Factors from {tr_path}")
    
    # Load in the student factors and attraction factors separately
    attraction_file = "Attraction Factors.txt"
    attraction_factors = np.loadtxt(os.path.join(factors_base, attraction_file))
    
    log_func("Loaded Attraction Factors with shape: %s" % str(attraction_factors.shape))
    
    # Read in planning data and pivoting files
    # planning data
    
    # If using home working split inputs, create 2 tmfs_array objects, one
    # for each split. These can be combined in create_production_pivot()
    if integrate_home_working:
        tel_scenario_tmfs = "{}_hw".format(tel_scenario.lower())
        # Need to load in extra columns for the working at home split
        use_cols_tmfs = range(2, 15)
        # Define how the array will be split - take 2 sets of columns
        split_tmfs = {"WAH": [0, 1, 2, 3, 4, 5, 6, 11, 12, 13, 14],
                      "WBC": [0, 1, 2, 7, 8, 9, 10, 11, 12, 13, 14]}
    else:
        tel_scenario_tmfs = tel_scenario.lower()
        use_cols_tmfs = range(2, 11)
        split_tmfs = None

    tel_tmfs_file = os.path.join(
        delta_root,
        tel_scenario,
        "tmfs%s%s.csv" % (tel_year, tel_scenario_tmfs)
    )
    tel_tav_file = os.path.join(delta_root, tel_scenario, 
                                 "tav_%s%s.csv" % (tel_year, tel_scenario.lower()))
    # base pivoting files
    base_tmfs_file = os.path.join(tmfs_root, "Runs", base_year, "Demand",
                                  base_id, "tmfs%s_%s.csv" % (base_year, base_id))
    base_tav_file = os.path.join(tmfs_root, "Runs", base_year, "Demand",
                                  base_id, "tav_%s_%s.csv" % (base_year, base_id))
    
    
    tmfs_base_array = np.loadtxt(base_tmfs_file, skiprows=1, delimiter=",")
    count_i = tmfs_base_array.shape[0]
    tav_base_array = np.loadtxt(base_tav_file, skiprows=1, delimiter=",")
    
    
    tav_array = np.loadtxt(tel_tav_file, skiprows=1, delimiter=",")
    count_tav = tav_array.shape[0]
    tmfs_array = np.loadtxt(tel_tmfs_file, skiprows=1, delimiter=",",
                            usecols=use_cols_tmfs)
    tmfs_array = np.concatenate((np.zeros_like(tmfs_array[:,[0,1]]), tmfs_array), axis=1)
    count_tmfs = tmfs_array.shape[0]

    # Extract the columns required for the 2 versions of tmfs_array if required
    if split_tmfs:
        tmfs_array = {
            work_type: tmfs_array[:, split_cols] 
            for work_type, split_cols in split_tmfs.items()
        }
        for work_type in tmfs_array:
            tmfs_array[work_type][:,4], tmfs_array[work_type][:,5] = (
                tmfs_array[work_type][:,5], tmfs_array[work_type][:,4].copy()
            )
    else:
        # Previous version swaps columns 4 and 5 of the planning data
        # to be in line with the tmfs07 version expects
        tmfs_array[:,4], tmfs_array[:,5] = (
            tmfs_array[:,5], tmfs_array[:,4].copy()
        )
    
    log_func("TAV Count: %d" % count_tav)
    log_func("I Count: %d" % count_i)
    log_func("TMFS Count: %d" % count_tmfs)

    # # # # # # # # # # # # # # # #
    # Put income segregation here #
    # (not implemented)           #
    # # # # # # # # # # # # # # # #

    # Rearrange and account for students
    if split_tmfs:
        tmfs_adj_array = {}
        # Adjust each array individually
        for work_type in tmfs_array:
            tmfs_adj_array[work_type] = student_factor_adjustment(
                tmfs_array[work_type]
            )
            tmfs_array_shape = tmfs_array[work_type].shape
            tmfs_adj_array_shape = tmfs_adj_array[work_type].shape
    else:
        tmfs_adj_array = student_factor_adjustment(tmfs_array)
        tmfs_array_shape = tmfs_array.shape
        tmfs_adj_array_shape = tmfs_adj_array.shape

    # tmfs_adj_array = np.copy(tmfs_array)
    # tmfs_adj_array[:,:3] = tmfs_adj_array[:,2:5]
    # tmfs_adj_array[:,3] = tmfs_adj_array[:,7] * MALE_STUDENT_FACTOR
    # tmfs_adj_array[:,4] = tmfs_adj_array[:,8] * FEMALE_STUDENT_FACTOR
    # tmfs_adj_array[:,7] = tmfs_adj_array[:,7] * (1 - MALE_STUDENT_FACTOR)
    # tmfs_adj_array[:,8] = tmfs_adj_array[:,8] * (1 - FEMALE_STUDENT_FACTOR)

    log_func("TAV Base Array shape = %s" % str(tav_base_array.shape))
    log_func("TMFS Array shape = {}".format(tmfs_array_shape))
    log_func("TMFS Adj Array shape = {}".format(tmfs_adj_array_shape))
    # Attraction Factors
    # Apply the attraction factors to the tav array planning data
    attr_file = os.path.join(tmfs_root, "Runs", tel_year, "Demand", tel_id,
                             "tav_%s_%s.csv" % (tel_year, tel_id))
    attr_factors_array = create_attraction_pivot(tav_array, attraction_factors)
    # Output pivot attraction factors
    np.savetxt(attr_file, attr_factors_array.round(3), delimiter=",",
            header="HW,HE,HO,HS", fmt="%.3f", comments="")
    log_func("Attraction Factors saved to %s" % str(attr_file))

    # Calculate Attraction Growth Factors
    attr_growth_array = calculate_growth(base=tav_base_array.round(3), forecast=attr_factors_array.round(3))

    # # # # # # # # # # # #
    # Production Factors
    area_corres_file = os.path.join(tmfs_root, "Factors", "AreaCorrespondence.csv")
    area_corres_array = np.loadtxt(area_corres_file, skiprows=1, delimiter=",",
                                   usecols=1, dtype="int8")
    # Area correspondence array maps tmfs18 zones to their urban rural classification
    area_corres_array = np.repeat(area_corres_array, 8)
    log_func("Area Correspondence shape = %s" % str(area_corres_array.shape))
    
    check_file = os.path.join(tmfs_root, "Runs", tel_year,
                              "Demand", tel_id, "check2.csv")
    
    # Create the production pivot data - multiplying population/planning data 
    # by the trip rates
    
    # For home working split data, we need to combine the resulting pivot data
    if integrate_home_working:
        prod_factor_array = None
        for work_type in tmfs_adj_array:
            # Check that the work type is valid (Should be WAH or WBC)
            if work_type not in p_trip_rate_array:
                raise ValueError("Error: Could not find split in Trip Rates")
        
            # Pick out the columns that have not been split
            split_cols = [1, 2, 5, 6]
            # Halve all other columns to prevent double counts
            split_pop_data = tmfs_adj_array[work_type].copy()
            non_split_cols = [x for x in range(split_pop_data.shape[1])
                              if x not in split_cols]
            split_pop_data[:, non_split_cols] /= 2
        
            temp_prod_factors = create_production_pivot(
                planning_data=split_pop_data,
                production_trip_rates=p_trip_rate_array[work_type],
                area_correspondence=area_corres_array,
                check_file=check_file,
                count_tav=count_tav,
                output_shape=tmfs_base_array.shape,
                just_pivots=just_pivots
            )
            
            if prod_factor_array is None:
                prod_factor_array = temp_prod_factors
            else:
                prod_factor_array += temp_prod_factors
    # Otherwise, can just use the output data
    else:
        prod_factor_array = create_production_pivot(
            tmfs_adj_array,
            p_trip_rate_array,
            area_corres_array,
            check_file,
            count_tav,
            tmfs_base_array.shape,
            just_pivots
        )
    
    
    prod_factor_file = os.path.join(tmfs_root, "Runs", tel_year, "Demand", tel_id, "tmfs%s_%s.csv" % (tel_year, tel_id))
    
    # Output pivot production factors
    file_header = ("WAC C0,WAC C11,WAC C12,WAC C2,WAP C0,WAP C11,WAP C12,"
                   "WAP C2,OAC C0,OAC C11,OAC C12,OAC C2,OAP C0,OAP C11,OAP C12,OAP C2,"
                   "EAC C0,EAC C11,EAC C12,EAC C2,EAP C0,EAP C11,EAP C12,EAP C2,SAC C0,"
                   "SAC C11,SAC C12,SAC C2,SAP C0,SAP C11,SAP C12,SAP C2,WIC C0,WIC C11,"
                   "WIC C12,WIC C2,WIP C0,WIP C11,WIP C12,WIP C2,OIC C0,OIC C11,OIC C12,"
                   "OIC C2,OIP C0,OIP C11,OIP C12,OIP C2,EIC C0,EIC C11,EIC C12,EIC C2,"
                   "EIP C0,EIP C11,EIP C12,EIP C2,SIC C0,SIC C11,SIC C12,SIC C2,SIP C0,"
                   "SIP C11,SIP C12,SIP C2")
    if just_pivots is True:
        # Output pm and offpeak factors 
        file_header += (",WPC C0,WPC C11,WPC C12,WPC C2,WPP C0,WPP C11,"
                   "WPP C12,WPP C2,OPC C0,OPC C11,OPC C12,OPC C2,OPP C0,OPP C11,OPP C12,"
                   "OPP C2,EPC C0,EPC C11,EPC C12,EPC C2,EPP C0,EPP C11,EPP C12,EPP C2,"
                   "SPC C0,SPC C11,SPC C12,SPC C2,SPP C0,SPP C11,SPP C12,SPP C2"
                   ",WOC C0,WOC C11,WOC C12,WOC C2,WOP C0,WOP C11,"
                   "WOP C12,WOP C2,OOC C0,OOC C11,OOC C12,OOC C2,OOP C0,OOP C11,OOP C12,"
                   "OOP C2,EOC C0,EOC C11,EOC C12,EOC C2,EOP C0,EOP C11,EOP C12,EOP C2,"
                   "SOC C0,SOC C11,SOC C12,SOC C2,SOP C0,SOP C11,SOP C12,SOP C2")
        np.savetxt(prod_factor_file, prod_factor_array.round(3), delimiter=",",
                   header=file_header, fmt="%.3f", comments="")
        log_func("Completed calculating pivoting tables")
        log_func("Finished")
        return

    np.savetxt(prod_factor_file, prod_factor_array.round(3), delimiter=",",
                header=file_header, fmt="%.3f", comments="")
    log_func("Production Factors saved to %s" % str(prod_factor_file))
        
    
    ## # # # # # # # # #
    # Production Growth Factors
    # Calculate growth from the base and tel_year pivot files
    prod_growth_array = calculate_growth(base=tmfs_base_array.round(3), forecast=prod_factor_array.round(3))
    log_func("Production Growth Array shape = %s" % str(prod_growth_array.shape))

    prefixes = ["AM","IP"]
    types = ["HWZ_A1", "HOZ_A1_ALL", "HEZ_A1_ALL", "HSZ_A1"]
    tod_files = ["%s_%s.TOD" % (prefix, t) for prefix in prefixes for t in types] + ["PM_HSZ_A1.TOD"]
    cte_files = [("%s_%s.CTE" % (prefix, t)).replace("A1","D0") for prefix in prefixes for t in types] + ["PM_HSZ_D0.CTE"]
    
    cte_tod_base_path = os.path.join(tmfs_root, "Runs", base_year,"Demand", base_id)
    tod_data, cte_data = load_cte_tod_files(tod_files, cte_files, cte_tod_base_path)
    
    log_func("TOD Data Shape = %s" % str(tod_data.shape))
    log_func("CTE Data Shape = %s" % str(cte_data.shape))

    airport_growth = np.ones(count_tav,dtype="float")
    if is_rebasing_run is False:
        # Airport indices are as follows (zones are indices + 1):
        #   708 = Edinburgh Airport
        #   709 = Prestwick Airport
        #   710 = Glasgow Airport
        #   711 = Aberdeen Airport
        
        #factors = pd.DataFrame([[708,1.05588],[709,1.0],[710,1.02371],[711,1.01213]])
        
        # Factors updated as of TMfS 2018
        #   708 = 4.97% Edinburgh 
        #   709 = 7.6%  Prestwick
        #   710 = 3.33% Glasgow
        #   711 = 1.86% Aberdeen
        
        ####
        #   Previous method for airport growth < TMfS18 (constant value per annum)
        #factors = pd.DataFrame([[708,1.0497],[709,1.076],[710,1.0333],[711,1.0186]])
        #airport_growth[factors[0]] = factors[1] ** (int(tel_year) - int(base_year))
        
        ####
        #   New method for airport growth >= TMfS18 (growth varies according to DfT 2017 aviation forecast)
        # now reads in a file from "Factors" that contains the expected growth 
        # from 2017
        if airport_growth_file == "":
            airport_growth_file = os.path.join(tmfs_root, "Factors", 
                                           "airport_factors.csv")
        if not os.path.isfile(airport_growth_file):
            raise FileNotFoundError("File does not exist: {}".format(
                    airport_growth_file))
        factors = pd.read_csv(airport_growth_file, index_col="Year")
        factors = factors.loc[int(tel_year) + 2000]/factors.loc[int(base_year) + 2000]
        airport_growth[factors.index.astype("int")] = factors.values

    sw_array, sw_cte_array = apply_pivot_files(tod_data, cte_data, 
                                             prod_growth_array.round(5), attr_growth_array.round(5),
                                             airport_growth)

    log_func("SW Array shape = %s" % str(sw_array.shape))

    # Print the TOD and CTE files - index +1: array(round(3))
    # All the zones are internal, so are labelled continuously - 1->787 as of tmfs18
    
    base_path = os.path.join(tmfs_root, "Runs", tel_year, "Demand", tel_id)
    
    save_trip_end_files(tod_files, sw_array, base_path, 3)
    log_func("TOD Files saved to %s" % str(base_path))
    save_trip_end_files(cte_files, sw_cte_array, base_path, 5)
    log_func("CTE Files saved to %s" % str(base_path))
    
    # Check that each array CTE and TOD is > 15kBytes
    for i,test_array in enumerate(sw_array):
        if test_array.nbytes < 15000:
            log_func("TOD Array is incomplete: %d" % i)
    for i,test_array in enumerate(sw_cte_array):
        if test_array.nbytes < 15000:
            log_func("CTE Array is incomplete: %d" % i)
            
